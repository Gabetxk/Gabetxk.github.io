{"cells":[{"cell_type":"markdown","id":"ef36f535-4bdc-4e2b-a22a-179372324b26","metadata":{},"source":["![walmartecomm](walmartecomm.jpg)\n","\n","Walmart is the biggest retail store in the United States. Just like others, they have been expanding their e-commerce part of the business. By the end of 2022, e-commerce represented a roaring $80 billion in sales, which is 13% of total sales of Walmart. One of the main factors that affects their sales is public holidays, like Thanksgiving, and Christmas. \n","\n","In this project, I have been tasked with creating a data pipeline for the analysis of supply and demand around the holidays, along with conducting a preliminary analysis of the data. I will be working with two data sources: grocery sales and complementary data. I have been provided with the `grocery_sales` table in csv format with the following features:\n","\n","# `grocery_sales`\n","- `\"index\"` - unique ID of the row\n","- `\"Store_ID\"` - the store number\n","- `\"Date\"` - the week of sales\n","- `\"Weekly_Sales\"` - sales for the given store\n","\n","Also, I have the `extra_data.parquet` file that contains complementary data:\n","\n","# `extra_data.parquet`\n","- `\"IsHoliday\"` - Whether the week contains a public holiday - 1 if yes, 0 if no.\n","- `\"Temperature\"` - Temperature on the day of sale\n","- `\"Fuel_Price\"` - Cost of fuel in the region\n","- `\"CPI\"` â€“ Prevailing consumer price index\n","- `\"Unemployment\"` - The prevailing unemployment rate\n","- `\"MarkDown1\"`, `\"MarkDown2\"`, `\"MarkDown3\"`, `\"MarkDown4\"` - number of promotional markdowns\n","- `\"Dept\"` - Department Number in each store\n","- `\"Size\"` - size of the store\n","- `\"Type\"` - type of the store (depends on `Size` column)\n","\n","I was requested to merge those files and perform some data manipulations. The transformed DataFrame can then be stored as the `clean_data` variable containing the following columns:\n","- `\"Store_ID\"`\n","- `\"Month\"`\n","- `\"Dept\"`\n","- `\"IsHoliday\"`\n","- `\"Weekly_Sales\"`\n","- `\"CPI\"`\n","- \"`\"Unemployment\"`\"\n","\n","After merging and cleaning the data, I will have to analyze monthly sales of Walmart and store the results of my analysis as the `agg_data` variable that should look like:\n","\n","|  Month | Weekly_Sales  | \n","|---|---|\n","| 1.0  |  33174.178494 |\n","|  2.0 |  34333.326579 |\n","|  ... | ...  |  \n","\n","Finally, saving the `clean_data` and `agg_data` as the csv files."]},{"cell_type":"code","execution_count":4,"id":"59fe49dc-cda5-4d22-bb10-49e94cdb6437","metadata":{"collapsed":true,"customType":"sql","dataFrameVariableName":"grocery_sales","executionCancelledAt":null,"executionTime":4439,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1751619591729,"lastExecutedByKernel":"144044bd-31a8-45de-8998-b935b4b3f47b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"-- Write your SQL query here\nSELECT * \nFROM grocery_sales","outputsMetadata":{"0":{"height":501,"tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"29feb1a4-0f40-4860-bb4a-a54e61fa99e1","nodeType":"const"}},"type":"dataFrame"}},"sqlCellMode":"dataFrame","sqlSource":{"integrationId":"89e17161-a224-4a8a-846b-0adc0fe7a4b1","type":"integration"}},"outputs":[],"source":["import pandas as pd\n","import os\n","def extract(store_data, extra_data):\n","    original_df = pd.read_csv(store_data)\n","    extra_df = pd.read_parquet(extra_data)\n","    merged_df = original_df.merge(extra_df, on = \"index\")\n","    return merged_df\n","\n","# Call the extract() function and store it as the \"merged_df\" variable\n","merged_df = extract(\"grocery_sales.csv\", \"extra_data.parquet\")"]},{"cell_type":"code","execution_count":5,"id":"6d3c25e2-e7d8-4c33-9be0-d45f03b2cf43","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1751619591862,"lastExecutedByKernel":"144044bd-31a8-45de-8998-b935b4b3f47b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create the transform() function with one parameter: \"raw_data\"\ndef transform(raw_data):\n  # Write your code here\n    num_cols = raw_data.select_dtypes(include=['number']).columns\n    raw_data[num_cols] = raw_data[num_cols].fillna(raw_data[num_cols].mean())\n    \n    raw_data['Month'] = pd.to_datetime(raw_data['Date']).dt.month\n\n    # Filter rows where 'Weekly_Sales' is over 10,000\n    raw_data = raw_data[raw_data['Weekly_Sales'] > 10000]\n\n    # Drop unnecessary columns (adjust this list as needed)\n    cols_to_drop = ['Date', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n                   'Fuel_Price', 'Temperature', 'Type', 'Size', 'index'] \n    raw_data = raw_data.drop(columns=cols_to_drop)\n\n    return raw_data"},"outputs":[],"source":["# Create the transform() function with one parameter: \"raw_data\"\n","def transform(raw_data):\n","  # Write your code here\n","    num_cols = raw_data.select_dtypes(include=['number']).columns\n","    raw_data[num_cols] = raw_data[num_cols].fillna(raw_data[num_cols].mean())\n","    \n","    raw_data['Month'] = pd.to_datetime(raw_data['Date']).dt.month\n","\n","    # Filter rows where 'Weekly_Sales' is over 10,000\n","    raw_data = raw_data[raw_data['Weekly_Sales'] > 10000]\n","\n","    # Drop unnecessary columns (adjust this list as needed)\n","    cols_to_drop = ['Date', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n","                   'Fuel_Price', 'Temperature', 'Type', 'Size', 'index'] \n","    raw_data = raw_data.drop(columns=cols_to_drop)\n","\n","    return raw_data"]},{"cell_type":"code","execution_count":6,"id":"620b7289-06cd-4205-be9e-a50dc8d36cf0","metadata":{"executionCancelledAt":null,"executionTime":89,"lastExecutedAt":1751619591951,"lastExecutedByKernel":"144044bd-31a8-45de-8998-b935b4b3f47b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Call the transform() function and pass the merged DataFrame\nclean_data = transform(merged_df)","outputsMetadata":{"0":{"height":80,"type":"stream"}}},"outputs":[],"source":["# Call the transform() function and pass the merged DataFrame\n","clean_data = transform(merged_df)"]},{"cell_type":"code","execution_count":7,"id":"b19b15e3-6624-47a9-927f-d3f12fe8212d","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1751619592002,"lastExecutedByKernel":"144044bd-31a8-45de-8998-b935b4b3f47b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\ndef avg_weekly_sales_per_month(clean_data):\n    # Write your code here\n    newdf = round(clean_data.groupby(['Month']).agg(Avg_Sales=('Weekly_Sales','mean')).reset_index(), 2)\n    return newdf"},"outputs":[],"source":["# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\n","def avg_weekly_sales_per_month(clean_data):\n","    # Write your code here\n","    newdf = round(clean_data.groupby(['Month']).agg(Avg_Sales=('Weekly_Sales','mean')).reset_index(), 2)\n","    return newdf"]},{"cell_type":"code","execution_count":8,"id":"fe875e27-b0cf-4e52-994e-4ae1fe6e8876","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1751619592054,"lastExecutedByKernel":"144044bd-31a8-45de-8998-b935b4b3f47b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\nagg_data = avg_weekly_sales_per_month(clean_data)\nprint(agg_data)","outputsMetadata":{"0":{"height":290,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["    Month  Avg_Sales\n","0     1.0   33174.18\n","1     2.0   34333.33\n","2     3.0   33240.16\n","3     4.0   33420.72\n","4     5.0   33335.33\n","5     6.0   34582.47\n","6     7.0   33922.76\n","7     8.0   33644.79\n","8     9.0   33258.06\n","9    10.0   32736.99\n","10   11.0   36594.03\n","11   12.0   39238.80\n"]}],"source":["# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\n","agg_data = avg_weekly_sales_per_month(clean_data)\n","print(agg_data)"]},{"cell_type":"code","execution_count":9,"id":"921cb123-3153-4334-bdeb-9bb227fdc530","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1751619592102,"lastExecutedByKernel":"144044bd-31a8-45de-8998-b935b4b3f47b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\ndef load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n    # Write your code here\n    full_data.to_csv(full_data_file_path, index = False)\n    agg_data.to_csv(agg_data_file_path, index = False)"},"outputs":[],"source":["# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\n","def load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n","    # Write your code here\n","    full_data.to_csv(full_data_file_path, index = False)\n","    agg_data.to_csv(agg_data_file_path, index = False)"]},{"cell_type":"code","execution_count":10,"id":"f518ad5c-214e-474b-80bd-827b0c0e1536","metadata":{"executionCancelledAt":null,"executionTime":387,"lastExecutedAt":1751619592489,"lastExecutedByKernel":"144044bd-31a8-45de-8998-b935b4b3f47b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Call the load() function and pass the cleaned and aggregated DataFrames with their paths    \nload(clean_data, 'clean_data.csv', agg_data, 'agg_data.csv')\n#print(clean_data)","outputsMetadata":{"0":{"height":311,"type":"stream"}}},"outputs":[],"source":["# Call the load() function and pass the cleaned and aggregated DataFrames with their paths    \n","load(clean_data, 'clean_data.csv', agg_data, 'agg_data.csv')\n","#print(clean_data)"]},{"cell_type":"code","execution_count":11,"id":"61b5f58a-70cb-40b3-bdbe-20b4079276e3","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1751619592538,"lastExecutedByKernel":"144044bd-31a8-45de-8998-b935b4b3f47b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\ndef validation(file_path):\n    # Write your code here\n    if os.path.isfile(file_path):\n        return 'File exists'\n    else:\n        return 'File not found'"},"outputs":[],"source":["# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\n","def validation(file_path):\n","    # Write your code here\n","    if os.path.isfile(file_path):\n","        return 'File exists'\n","    else:\n","        return 'File not found'"]},{"cell_type":"code","execution_count":12,"id":"df1659ff-41c4-4a92-9812-80c6eaa02b90","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1751619592586,"lastExecutedByKernel":"144044bd-31a8-45de-8998-b935b4b3f47b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\nvalidation('clean_data.csv')\npass\nvalidation('agg_data.csv')"},"outputs":[{"data":{"text/plain":["'File exists'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\n","validation('clean_data.csv')\n","pass\n","validation('agg_data.csv')"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"editor":"DataLab","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}
